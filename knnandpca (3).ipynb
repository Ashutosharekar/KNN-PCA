{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KNN & PCA Assignment\n",
        "\n",
        "1. What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        " - K-Nearest Neighbors (KNN) is a supervised, non-parametric, instance-based learning  algorithm.\n",
        "    It works by finding the K closest data points to a new input based on a distance metric such as Euclidean distance.\n",
        "    \n",
        "    In classification, the class is decided by majority voting among the K nearest neighbors.\n",
        "    In regression, the output is the average of the target values of the K nearest neighbors.\n",
        "    KNN does not build an explicit model and stores the entire training dataset.\n",
        "    Its performance heavily depends on the choice of K and distance metric.\n",
        "\n",
        "2. What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        " - The Curse of Dimensionality refers to problems that arise when data has too many features (dimensions).As dimensions increase, data points become sparse, and distances between points become less meaningful.\n",
        "    \n",
        "    In KNN, this makes it difficult to identify true nearest neighbors.\n",
        "    The algorithm requires more data to maintain performance in high dimensions.\n",
        "    It also increases computation time and memory usage.\n",
        "    Dimensionality reduction techniques like PCA are commonly used to mitigate this issue.\n",
        "\n",
        "3. What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        " - Principal Component Analysis (PCA) is an unsupervised dimensionality reduction technique.It transforms original features into a smaller set of new orthogonal features called principal components.These components capture the maximum variance in the data.\n",
        "    \n",
        "    Feature selection chooses a subset of original features, while PCA creates new features.PCA may reduce interpretability but improves performance and reduces noise.Feature selection preserves original feature meaning but may miss hidden patterns.\n",
        "\n",
        "4. What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        " - Eigenvectors represent the direction of maximum variance in the data.\n",
        "    \n",
        "    Eigenvalues indicate the amount of variance captured by each eigenvector.\n",
        "    \n",
        "    In PCA, eigenvectors become principal components.\n",
        "    Eigenvalues help rank components based on importance.\n",
        "    Components with higher eigenvalues are retained, while others are discarded.\n",
        "    This allows effective dimensionality reduction with minimal information loss.\n",
        "\n",
        "5. How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        " - PCA reduces the dimensionality of data, removing noise and redundant features.\n",
        "This directly improves KNN performance by making distance calculations more meaningful.\n",
        "Lower dimensions reduce computational cost and overfitting risk.\n",
        "KNN benefits from cleaner, compact feature space.\n",
        "Together, PCA + KNN handle high-dimensional datasets effectively.\n",
        "This combination is widely used in real-world ML pipelines.\n",
        "\n"
      ],
      "metadata": {
        "id": "AyLq6xQaL7SU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "6. Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.\n",
        "'''\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Without scaling\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "print(\"Accuracy without scaling:\", accuracy_score(y_test, knn.predict(X_test)))\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "knn.fit(X_train_s, y_train)\n",
        "print(\"Accuracy with scaling:\", accuracy_score(y_test, knn.predict(X_test_s)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFQ-EpA9MGjW",
        "outputId": "5880aebb-2070-49c0-e233-fa91166e9313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7222222222222222\n",
            "Accuracy with scaling: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "7. Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component.\n",
        "\n",
        "'''\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "print(\"Explained Variance Ratio:\")\n",
        "print(pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXqqc_9aMcUK",
        "outputId": "d3ae671f-b143-44e1-bc19-3629649a2302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio:\n",
            "[9.98091230e-01 1.73591562e-03 9.49589576e-05 5.02173562e-05\n",
            " 1.23636847e-05 8.46213034e-06 2.80681456e-06 1.52308053e-06\n",
            " 1.12783044e-06 7.21415811e-07 3.78060267e-07 2.12013755e-07\n",
            " 8.25392788e-08]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset.\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy with PCA:\", accuracy_score(y_test, knn.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyEh2Ta0Mfdn",
        "outputId": "b85810f4-a82b-4086-c3e6-b62bd102fe08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with PCA: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "9.  Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "print(\"Accuracy with Euclidean Distance:\", knn_euclidean.score(X_test, y_test))\n",
        "\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "print(\"Accuracy with Manhattan Distance:\", knn_manhattan.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSmgHaKmMiTK",
        "outputId": "2decf5d5-3e75-4313-fba9-b7f223067004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean Distance: 0.9444444444444444\n",
            "Accuracy with Manhattan Distance: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "\n",
        "Explain how you would:\n",
        "1.  Use PCA to reduce dimensionality\n",
        "2.  Decide how many components to keep\n",
        "3. Use KNN for classification post-dimensionality reduction\n",
        "4. Evaluate the model\n",
        "5. Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "\n",
        "\n",
        "-  In high-dimensional biomedical data, models often overfit due to many features and few samples.\n",
        "    \n",
        "    We first apply PCA to reduce dimensionality while preserving maximum variance.\n",
        "    \n",
        "    The number of components is chosen using explained variance (e.g., 95%).\n",
        "    \n",
        "    KNN is then trained on the reduced feature space to improve generalization.\n",
        "    \n",
        "    Model evaluation is done using cross-validation and accuracy or F1-score.\n",
        "    \n",
        "    This pipeline reduces noise, improves stability, and is computationally efficient.\n",
        "    It provides a robust, interpretable, and scalable solution suitable for real-world medical applications.    \n"
      ],
      "metadata": {
        "id": "Cr2QjYTSNfYG"
      }
    }
  ]
}